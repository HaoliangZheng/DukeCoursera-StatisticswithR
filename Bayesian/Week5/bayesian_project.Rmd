## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
library(BAS)
library(corrplot)
library(GGally)
library(gridExtra)
```

### Load data


```{r load-data}
load("movies.Rdata")
```



* * *

## Part 1: Data

In this project, the research is based on the data of 651 movies produced and released before 2016. 

As we can learn from the codebook, the data is from a random sample, so it is **generalizable**. Also, it's obvious that it's just an observation study and the random assignment was **not** used, so we can't get any causality result from this sample.



* * *

## Part 2: Data manipulation

Before starting my research, I need to be familiar with the data set. First I check the codebook, then I explore the data by using the `glimpse()` to investigate the structure.

```{r structure}
glimpse(movies)
```

We can learn that there are 651 movies and 32 variables including both numerical and categorical.


According to the instructions, all the variables we need are shown below.

```{r data1}
res = c(18)
tim = c(7,8)
cat = c(2,3)
num = c(4,13,14,16)
ord = c(5,19,20,21,22,23,24)

movies1 <-  movies[,c(res,tim,cat,num,ord)]

movies1 <- movies1 %>%
  filter(!is.na(runtime))
```

The object movies1 is a much cleaner dataset than movies.

Also, according to the instructions, we should create 5 new variables based on the existing ones.

- `feature_film`

```{r feature_film}
movies1 <- movies1 %>%
  mutate(feature_film = ifelse(title_type == 'Feature Film', 'yes', 'no'))
```

- `drama`

```{r drama}
movies1 <- movies1 %>%
  mutate(drama = ifelse(genre == 'Drama', 'yes', 'no'))
```

- `mpaa_rating_R`

```{r mpaa_rating_R}
movies1 <- movies1 %>%
  mutate(mpaa_rating_R = ifelse(mpaa_rating == 'R', 'yes', 'no'))
```

- `oscar_season`

```{r oscar_season}
movies1 <- movies1 %>%
  mutate(oscar_season = ifelse(thtr_rel_month %in% c(10,11,12), 'yes', 'no'))
```

- `summer_season`

```{r summer_season}
movies1 <- movies1 %>%
  mutate(summer_season = ifelse(thtr_rel_month %in% c(5,6,7,8), 'yes', 'no'))
```

Finally, let's clean the dataset again by removing the used variables.

```{r data2}
movies2 <- movies1[,-c(3,4,5,10)]
```

* * *

## Part 3: Exploratory data analysis

Here, I'm going to check collinearity and the relationship between response and predictors with *correlation plots*, *generalized pairs plots* and *dide-by-side box plots* below.

**Correlation Plots**

For numerical predictors, I use **Pearson correlation** to measure the degree of those variables correlated. A correlation matrix and a correlation plot are shown below.

```{r corrplot1}
cor(movies2[,2:6])
corrplot(cor(movies2[,2:6]))
```

We can see that `critics_score` and `imdb_rating` are highly positive correlated, which is quite reasonable. To avoid the collinearity and using 'popularity' to predict 'popularity', we'd better delete `imdb_rating` variable.

**Generalized Pairs Plots**

To see whether there is association between response variable and other numerical variables, then I use generalized pairs plots.

```{r ggpairs1, warnings=FALSE, message=FALSE, prompt=FALSE, comment=NA}
ggpairs(data = movies2, columns = 1:6)
```

From this graph, we can see that the scatter plot of `audience_score` vs `thtr_rel_year` shows no pattern at all, indicating that we can delete the `thtr_rel_year` variable.


**Side-by-side Box Plots**

At last, I use side-by-side box plots also to see whether there is association between response variable and other categorical variables.

```{r boxplots}
ord_ind = c(7:17)
ord_nam = names(movies2)[ord_ind]

for(i in 1:11){
  temmov <- movies2[,c(1,ord_ind[i])]
  plots <- NULL
  plots <- ggplot(data = temmov, aes_string(x = ord_nam[i], y = 'audience_score')) + 
    geom_boxplot() +
    labs(x = names(temmov)[2]) + 
    theme(legend.position='none')
  assign(paste0("plots",i),plots)
}

grid.arrange(plots1, plots2, plots3, plots4, plots5, plots6, plots7, plots8, plots9, plots10, plots11, nrow=4, ncol=3)
```

From this graph, we can learn some interesting facts as follow. First, for variables `best_pic_nom`, `best_pic_win`, `top200_box` and `feature_film`, there shows strong difference between the averages. Second, for variables `best_actor_win` and `best_actress_win`, there shows no strong difference between the averages, indicating that maybe we can get rid of these variables.


Due to the result in this part, I'd rather delete `imdb_rating`, `thtr_rel_year`, `best_actor_win` and `best_actress_win` variables to get a simpler full model.

* * *

## Part 4: Modeling

According to the analysis above, the variables included in the full model are response variable `audience_score`, and other predictors like `runtime`, `imdb_num_votes`, `critics_score`, etc, 12 variables.

```{r data3}
movies3 <- movies2[,-c(2,4,9,10)]
```

**Model Uncertainty**

The data I'm going to use is all in the movies3. Now let's build our models by using the `bas.lm()` and set modelprior to be uniform(). We can get information of models through posterior inclusion probabilities and posterior probability of each model.

```{r models}
movies.models <- bas.lm(audience_score ~ ., data = movies3, prior = "BIC", modelprior = uniform())

movies.models

summary(movies.models)
```

From the regression table, we can learn lots of interesting facts as follow. First, the pip of `imdb_num_votes`, `critics_score` and `feature_film` variables are almost 1, and the pip of `drama` is greater than 0.5. The pip of other variables are quite small compared to these ones. Second, the model 1 has quite large posterior probabilities than other models, and it includes all variables whose pip are greater than 0.5.

```{r image}
image(movies.models, rotate = F)
```

Also, by using the `image` function, we can create an image of the model space, which also shows that model 1 will be our best choice.

**Best Model**

From the former step, we can find that model 1 is not only the highest probability but also the median probability model. So my best model will be model 1, and the chosen variables are shown below.

```{r best_gamma}
best = which.max(movies.models$logmarg)
bestmodel = movies.models$which[[best]]
bestgamma = rep(0, movies.models$n.vars)
bestgamma[bestmodel + 1] = 1

bestgamma
```

Now since we will only provide one model, which is our best model, we place all model prior probability to this exact model. Because we want to fit only using variables in model 1, we use data = movies3[,c(1,3,4,9,10)] to indicate which predictors are included. The argument n.models = 1 fits just this one model.

```{r best_model}
movies.best = bas.lm(audience_score ~ ., data = movies3[,c(1,3,4,9,10)], prior = "BIC", n.models = 1, bestmodel = rep(1,5), modelprior = uniform())

movies.best
coef(movies.best)
```


**Model Diagnostics**

In this step, I'm going to check model conditions using graphs.

- Linearity
- Constant variability

```{r check_linearity}
plot(movies.best, which = 1, add.smooth = F, ask = F)
```

We can see that residuals seem to randomly scatter around the dashed line and show no sign of a fan shape. This tells us that everything is all right with these two conditions.

- Nearly normal residuals

```{r check_normal}
movies.p = fitted(movies.models, estimator = "BMA")
movies.fit = movies.p
movies.res = movies3$audience_score - movies.fit

ggplot(data = data.frame(movies.res), aes(x = movies.res)) +
  geom_histogram(binwidth = 5) +
  xlab("Residuals")
```

We can see that the graph seem to be symmetric and nearly normal. This tells us that everything is all right with this condition.

**Interpretation**

It is possible to visualize the posterior distribution of the coefficients, and I graph the posterior distribution of the coefficients below.

```{r plot_pip}
movies.coef <- coefficients(movies.best)
op = par(mfrow = c(2,2))
plot(movies.coef, subset = c(2,3,4,5), ask = FALSE)
```

The probability of being 0 is quite small, the credible intervals of coefficients will also tell us the same information.

```{r confint}
confint(movies.coef)
```

According to the credible intervals above, we can see that: all other factors held even, there are 95% probability that, 1 more `imdb_num_votes` will increse the `audience_score` by 2.27e-05 to 4.27e-05, 1 more `critics_score` will increse the `audience_score` by 3.90e-01 to 4.75e-01, `feature_film` being "yes" will decrese the `audience_score` by 6.06 to 14.59, `drama` being "yes" will increase the `audience_score` by 0.85 to 5.43. 

* * *

## Part 5: Prediction

The movie I chose to predict is a documentary called **“The Beatles: Eight Days a Week - The Touring Years” (2016)**, whose `audience_score` is **89**.

First, we need to create a new data frame for this movie.

```{r new_movie}
newmovie <- data.frame(imdb_num_votes = 10830, critics_score = 96, feature_film = "no", drama = "no")
```

Then, I can do the prediction using the `predict` function.

```{r new_movie_predict}
newmovies.pre = predict(movies.best, newmovie, se.fit = TRUE)
newmovies.pre
```

We can see that my prediction is 85, close to the true value 89. Due to the uncertainty around this prediction, we can also construct a prediction interval.

```{r new_movie_predict_interval}
confint(newmovies.pre)
```

We can see that 95% prediction interval is (58,112), containing the true value 89, which tells us the model is working.

* * *

## Part 6: Conclusion

Through fitting and predicting, we have learned some attributes make a movie popular or not, which are `imdb_num_votes`, `critics_score`, `feature_film` and `drama`.

A new movie regarding the Beatles is used to predict, and the result is quite good, which shows that my model performs all right.

When it comes to **shortcomings**, I think some variables may be usefel are omitted due to my lack of ability to handle them. Also, maybe there is non-linear association, and I haven't considered this question.